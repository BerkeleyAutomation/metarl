import numpy as np 
from MultiAgentPolicy import MultiAgentPolicy 
from sklearn.linear_model import SGDClassifier
from numpy.random import multinomial
import logging

class MAgentMultinomial(MultiAgentPolicy):

    """Uses a multinomial generated by the confidence from previous transitions"""
    def __init__(self, representation, subagents, **kwargs):
        logging.getLogger().setLevel(logging.INFO)
        self._s = None
        self._a = None
        super(MAgentMultinomial, self).__init__(representation, subagents, **kwargs)
        self.dim = self.subagents[0].representation.state_space_dims

    def pi(self, s, terminal, p_actions):

        if self.representation.domain.restarted:
            # print "Restarted at ", s

            action = self.random_state.choice(p_actions)
        else:         
            probabilities = self.get_subagent_transition_probabilities(self._s, self._a, s)
            normalized = probabilities / sum(probabilities)

            chosen_agent = np.argwhere(multinomial(1, normalized))[0, 0]
            action = self.subagents[chosen_agent].policy.pi(s[:self.dim], terminal, p_actions)

            logging.debug(" >>>>>>>>>>>>>>>>>>>>> ")
            logging.debug("Previous state: {}".format(self._s))
            logging.debug("Action taken: %d", self._a)
            logging.debug("Current State: {}".format(s))
            logging.debug("Probabilities for each agent {}".format(normalized))
            logging.debug("Chosen agent: %d, Action: %d" % (chosen_agent, action))
        self._s = s
        self._a = action 
        return action

    def get_subagent_transition_probabilities(self, s, a, ns):
        return [agent.policy.get_confidence(s[:self.dim],a,ns[:self.dim]) for agent in self.subagents]



class MultiTerrainPolicy(MultiAgentPolicy):

    prev_state_votes = []
    votes = []
    def __init__(self, representation, subagents, **kwargs):
        self.classifier = SGDClassifier(loss='hinge', penalty='l2')

        super(MultiTerrainPolicy, self).__init__(representation, subagents, **kwargs)

    def show_learning(self, agent):
        domain = self.representation.domain
        learned = np.ones((domain.ROWS, domain.COLS)) * 9
        for y in np.r_[:domain.COLS]: #left right
            for x in np.r_[:domain.ROWS]: # up down
                if domain.check_valid_state([x, y]):
                    s = domain.terrain_state([x, y])
                    a = self.subagents[agent].policy.pi(s[:2], False, domain.possibleActions(s=[x, y]))
                    sa = np.append(s, [a, agent])
                    learned[x, y] = self.classifier.predict(sa.reshape(1, -1))
        print learned

    def pi(self, s, terminal, p_actions):
        """ 
        Chooses an action, first filtered by classification
        s: State augmented by dynamics
        """
        dim = self.subagents[0].representation.state_space_dims
        actions = np.array([agent.policy.pi(s[:dim], terminal, p_actions) 
                                        for agent in self.subagents])
        self.prev_state_votes = self.votes
        self.votes = (s, actions)
        # print "[INFO] Votes from given state:", self.votes 

        s_a_agents = [np.append(s, [a, agent]) for agent, a in enumerate(actions)]
        if not hasattr(self.classifier, 'coef_') or self.classifier.coef_ is None:
            return self.random_state.choice(actions) # HACK
        # self.show_learning(1)

        good_agents = [sa[-1] for sa in s_a_agents if self.classifier.predict(sa.reshape(1, -1))]

        bad = [sa[-1] for sa in s_a_agents if not self.classifier.predict(sa.reshape(1, -1))]
        
        # if len(bad) == 1 and bad[0] == 1:
        #     print "Current state, action, agents are ", s_a_agents
        #     print "Filtering these agents as bad", bad

        try:
            good_actions = actions[good_agents]
            a = max(set(good_actions), key=good_actions.tolist().count)
        except ValueError:
            a = self.random_state.choice(actions)
        
        return a


    def get_subagent_transition_probabilities(self, s, a, ns):
        return [agent.policy.get_confidence(s,a,ns) for agent in self.subagents]



class MultiTerrainQPolicy(MultiAgentPolicy):
    """
    Discontinued, as Q and V are consistent if both are null"""

    def __init__(self, representation, subagents, **kwargs):
        assert False
        self.classifier = SGDClassifier(loss='hinge', penalty='l2')
        super(MultiTerrainQPolicy, self).__init__(representation, subagents, **kwargs)

    def pi(self, s, terminal, p_actions):
        """Returns the best action as dictated by the classifier"""
        dim = self.subagents[0].representation.state_space_dims
        q_values = [agent.representation.Qs(s[:dim], terminal) for agent in self.subagents]
        best_actions = [np.argmax(qs) for qs in q_values]
        self.discrep = not all(action == best_actions[0] for action in best_actions)
        if not hasattr(self.classifier, 'coef_') or self.classifier.coef_ is None:
            return max(best_actions)
        return best_actions[self.classifier.predict(s.reshape(1, -1))]